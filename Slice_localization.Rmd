---
title: "Relative location of CT slices - MScA 32014"
author: "Ye"
date: "7/26/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## I. Peek at Data
In this project, we aim to use different regreesion methods (linear model, PCA method, Lasso method, and regression tree method) to predict the location of the slice using the features. A comparison between 4 methods is conducted in the last section.

Load librairies...
```{r}
suppressWarnings(library ('glmnet'))
suppressWarnings(library ('rpart'))
suppressWarnings(library ('rpart.plot'))
dataPath <- "~/Documents/Lecture/MachineLearning/ML_PA_SM17_Yuri/HW/project"
data <- read.csv(paste(dataPath,"slice_localization_data.csv",sep="/"),header=T)
coln = colnames(data)
nobs = nrow(data)
head(coln)
print(dim(data))
```

The data contain 384 features and 53,500 observations. Each record characterizes a slice of an image. Each CT slice is described by two histograms in polar space. The first histogram describes the location of bone structures in the image, the second the location of air inclusions inside of the body. Both histograms are concatenated to form the final feature vector of length 384. The class output variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body.

The column information:

1: PatientId: Each ID identifies a different patient

2 - 241: Histogram describing bone structures

242 - 385: Histogram describing air inclusions

386: Reference: Relative location of the image on the axial axis (class value). Values are in the range [0; 180] where 0 denotes the top of the head and 180 the soles of the feet.

```{r}
# define the predictors (drop the patient ID) and response
pred = data[,c(-1, -386)]
Y = data[, 386]
# look at the distribution of Y
plot(Y)
summary(Y)
```

## II. Linear Model

In this section, we apply a linear model to fit the data. We first fit with all predictors. Based on the regression result, we remove predictors insignificant with 5% level and predictors returning NA as coefficients and fit the reduced linear model.

```{r}
newdata = data.frame(Y=Y, pred)
lm.fit = lm(Y~., data=newdata)
# Result of linear model
res.lm = c(AIC=AIC(lm.fit), 
        Rsq = summary(lm.fit)$r.squared, 
        MSE = mean(lm.fit$residuals^2), 
        Npred = ncol(pred))
print(res.lm)
# Drop NA predictors
reducedData = newdata[,-which(is.na(lm.fit$coefficients))]
# Drop predictors insignificant with 5 % level
criteria = coefficients(summary(lm.fit))[,4] < 0.05
# Always keep the response
criteria[1] = TRUE
lmReduced = lm(Y~., data=reducedData[,criteria])
res.lm_red = c(AIC=AIC(lmReduced), 
           Rsq = summary(lmReduced)$r.squared, 
           MSE = mean(lmReduced$residuals^2), 
           Npred = length(lmReduced$coefficients) - 1)
print(res.lm_red)
```

After removing insignificant predicators with 5% level, the squared R decreases and AIC increases.

```{r}
#Examine the residual of the fit
hist(lmReduced$residuals)
qqnorm(lmReduced$residuals)
qqline(lmReduced$residuals)
```

The distribution of residuals of the reduced linear model has heavier tails than normal distribution. 

## III. PCA Method
In this section we apply a PCA regression method to fit the data. 
```{r}
# Apply pca
PCA.comp = prcomp(pred)
factorScores = PCA.comp$x
# Apply linear regresssion on metafeatures
PCA.fit<-lm(Y~.,data=data.frame(Y=Y, factorScores))
coef.pca = coefficients(summary(PCA.fit))
# Select significant metafeatures with 5 % level and apply again the linear regression
PCA.reduced<-lm(Y~.,data=data.frame(Y=Y, factorScores[,coef.pca[2:385, 4]<0.05]))
res.pca = c(AIC=AIC(PCA.reduced), 
           Rsq = summary(PCA.reduced)$r.squared, 
           MSE = mean(PCA.reduced$residuals^2), 
           PredNum = length(PCA.reduced$coefficients)-1)
print(res.pca)
hist(PCA.reduced$residuals)
qqnorm(PCA.reduced$residuals)
qqline(PCA.reduced$residuals)
```

After applying PCA, the distribution of residuals still have heavier tail, but is more close normal distribution than the reduced linear model.

## IV. Lasso Method
In this section we apply a Lasso regression method to fit the data. 

```{r}
# Use cross validation to find the optimal lambda for Lasso regression
cv.out=cv.glmnet(x=data.matrix(pred),y=Y,alpha=1)
plot(cv.out)
(bestlam =cv.out$lambda.min)
# Fit the data with optimal lambda
lasso.fit=glmnet(x=as.matrix(pred),y=Y,alpha=1,lambda=bestlam)
res.lasso = c(AIC=0, 
           Rsq = lasso.fit$dev.ratio, 
           MSE = deviance(lasso.fit)/nobs, 
           PredNum = lasso.fit$df)
print(res.lasso)
lasso.pred=predict(lasso.fit,s=bestlam,newx=data.matrix(pred))
lasso.resid = Y - lasso.pred
hist(lasso.resid)
qqnorm(lasso.resid)
qqline(lasso.resid)
```

## V. Regression Tree Method
In this section we apply a regression tree method to fit the data with 10-fold cross validation.

```{r}
# Use 10-fold cross validation to find the optimal lambda for Lasso regression 
tree = rpart(Y~., data=newdata, xval = 10)
print(tree$cptable)
```

The cross-validation error is minimized with the smallest cp in the table, which is 0.10. Therefore, there is no need to prune the tree.

```{r}
prp(tree, extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE)

tree.mse = mean(resid(tree)^2)
tree.Rsq = 1 - sum(resid(tree)^2) /sum((Y - mean(Y))^2) 
length(tree$variable.importance)
print('Summary for Lasso regression:')
res.tree = c( AIC = 0,
              Rsq = tree.Rsq, 
              MSE = tree.mse, 
              PredNum = length(tree$variable.importance))
print(res.tree)
hist(resid(tree))
qqnorm(resid(tree))
qqline(resid(tree))
```

## VI. Conclusion
```{r}
res = rbind(Linear = res.lm_red, 
            PCA = res.pca, 
            Lasso = res.lasso, 
            Tree = res.tree)
print(res)
```

In conclusion, we compare here the performance of four different regression methods. Lasso regression method has the highest squared R, mainly because it uses the most of predictors. PCA regression method has the worst performance, even though it has more number of predictors than tree.