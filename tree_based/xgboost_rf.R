suppressWarnings(library(xgboost))
suppressWarnings(library(randomForest))
dataPath = "~/Documents/Lecture/MachineLearning/ML_PA_SM17_Yuri/HW/hw7"
train <- read.csv(paste(dataPath,"train_sample.csv",sep="/"),header=T)
test <- read.csv(paste(dataPath,"test_sample.csv",sep="/"),header=T)

m = ncol(train)
predictors = paste0("X", 1:(m-1))
target = train$class
set.seed(1)
params = list("objective" = "binary:logistic")
data.mat = data.matrix(train[predictors])
cv = xgb.cv(params = params, data = data.mat, label = target, 
            nfold = 5, nrounds = 50, prediction = T, verbose = F)
cv.table = cv$evaluation_log
bestNR = which.min(cv.table$test_error_mean)
set.seed(1)
xg.model = xgboost(data = data.mat, label = target, params = params, nrounds = bestNR, verbose = F)
xg.error = mean((target - round(predict(xg.model, newdata=data.mat)))^2)
test.mat = data.matrix(test[predictors])
xg.pred = round(predict(xg.model, newdata=test.mat))

set.seed(1)
y = as.factor(train$class)
rf.model <- randomForest(x = data.mat, y, ntree=500, importance=TRUE)
print(rf.model)
importance(rf.model)
varImpPlot(rf.model, main="Variable Importance")
most = 6
pred = data.matrix(cbind(test$id, xg.pred))
saveRDS(list(RFMostImportant = most, Forecast = pred), "W7answer.rds")